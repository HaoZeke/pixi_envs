* About
Helper for integrating ~metatomic~ into GROMACS.

The primary development repository functions as a GitLab fork. Contributors should clone and work with this fork for all major development:
#+begin_src bash
git clone https://gitlab.com/HaoZeke/gromacs.git
#+end_src
- This fork connects with the upstream GitLab GROMACS for an eventual "MR," or merge request.
- The project also maintains a GitHub mirror on the =metatensor= organization.
  + Developers may submit pull requests, or "PRs" to this mirror.

Regardless of where a contribution begins, a single merge request for
=metatomic= proceeds through the GitLab fork to the GROMACS upstream repository.
** Usage
*** CUDA builds (local workstation)
#+begin_src bash
pixi r -e metatomic-cuda gromk-tmpi Release
cd mta_test
python create_model.py
pytest
#+end_src

*** CPU builds (CI / no GPU)
Thread-MPI (binary: ~gmx~):
#+begin_src bash
pixi r -e metatomic-cpu gromk-tmpi Release
cd mta_test && python create_model.py
GMX_BIN=gmx pytest -k "not dd8 and not dd12"
#+end_src

Real MPI (binary: ~gmx_mpi~):
#+begin_src bash
pixi r -e metatomic-cpu gromk-mpi Release
cd mta_test && python create_model.py
GMX_BIN=gmx_mpi pytest -k "not dd8 and not dd12"
#+end_src

Build directories are suffixed by device (~build-tmpi-cuda~, ~build-mpi-cpu~,
etc.) so CUDA and CPU builds coexist. The test suite in ~mta_test/~ is
device-agnostic; there are no CUDA-specific tests.

** Benchmarks
We use [[https://asv.readthedocs.io/][ASV (Airspeed Velocity)]] to track DD scaling performance.
Configuration and benchmark code live in ~gromacs/asv.conf.json~ and
~gromacs/benchmarks/~, following the [[https://github.com/TheochemUI/eOn][eOn]] layout.

*** Suite
| Class | Config | Measures |
|-------+--------+----------|
| ~TimeSerialMD~ | 1 rank | Baseline (no MPI) |
| ~TimeDD4~ | 4 ranks | 4-rank DD scaling |

Each class has ~time_*~ (wall-clock) and ~peakmem_*~ (peak RSS) methods.
Input data: 125-atom LJ system from ~mta_test/~.

*** Running locally
#+begin_src bash
# Build first
pixi r -e metatomic-cpu gromk-tmpi Release

# Install ASV
pixi run -e metatomic-cpu pip install asv

# Run from gromacs/ subdir
cd gromacs
pixi run -e metatomic-cpu bash -c 'asv machine --yes'

GMX_BIN=$(realpath ../.pixi/envs/metatomic-cpu/bin/gmx) \
  pixi run -e metatomic-cpu bash -c \
  'asv run -E "existing:$(which python)" --set-commit-hash $(git rev-parse HEAD) --record-samples --quick'
#+end_src

*** Comparing two commits
#+begin_src bash
MACHINE=$(ls .asv/results/ | grep -v benchmarks.json | head -1)
uvx --from "asv-spyglass @ git+https://github.com/HaoZeke/asv_spyglass.git@enh-multiple-comparisons" \
  asv-spyglass compare --label-before before --label-after after \
  .asv/results/$MACHINE/<hash1>*.json \
  .asv/results/$MACHINE/<hash2>*.json
#+end_src

*** CI
PRs to the GROMACS fork trigger two workflows:
- ~ci_benchmark.yml~: builds base + PR, runs ASV on both, uploads results
- ~ci_bench_commenter.yml~: posts comparison table as a PR comment (via ~asv-spyglass~)

*** Water Scaling: GROMACS vs LAMMPS

Benchmark comparing GROMACS metatomic (CPU IForceProvider) against LAMMPS pair_metatomic.
Located in =mta_bench/water_scaling/=.

**** Quick start

#+begin_src bash
# Prepare water boxes (needs gmx in PATH)
cd mta_bench/water_scaling
python prepare.py

# Run GROMACS (500 steps, with profiling timer):
cd gmx_cpu/216
GMX_METATOMIC_TIMER=1 $GMX_BIN mdrun -s topol.tpr -ntomp 1 -ntmpi 1 \
  -g md.log -pin off -nsteps 500

# Run LAMMPS (500 steps, with profiling timer):
cd ../../lmp/216
LAMMPS_METATOMIC_PROFILE=1 lmp -in lmp.in 2>lmp_timer.log

# Parse & plot (uses uv inline script deps):
cd ../..
python parse_timers_to_csv.py \
  --gmx-timer results/gmx_timer_216_500.log \
  --lmp-timer results/lmp_timer_216_500.log \
  -o results/timer_data.csv
uv run analyze_warmup.py \
  --gmx-timer results/gmx_timer_216_500.log \
  --lmp-timer results/lmp_timer_216_500.log \
  -o results/warmup_analysis.png
#+end_src

**** Results (216 water, 648 atoms, PET-MAD v1.0.2, RTX 4070 Ti SUPER)

| Metric                | GROMACS   | LAMMPS    |
|-----------------------+-----------+-----------|
| Steady-state (ms/step)| 55.0      | 79.8      |
| Forward (ms)          | 18.8      | 37.3      |
| Backward (ms)         | 33.1      | 33.8      |
| Other (NL, transfer)  | 3.1       | 8.7       |
| Warmup (steps 0-9)    | ~5500 ms  | ~5000 ms  |

GROMACS is 1.45x faster at steady state because it passes only 648 atoms
(no PBC ghosts) to the model, vs LAMMPS's 6727 atoms (648 + 6079 ghosts).
Both codes filter NL pairs to ~24K by the model cutoff.

** CI
The GROMACS fork at =gromacs/.github/workflows/metatomic-ci.yml= runs
CPU-only integration tests on every push to =metatomic= and on PRs. It
checks out =pixi_envs= alongside the GROMACS source and builds both
thread-MPI and real-MPI variants. Tests that need many ranks (dd8, dd12)
are skipped in CI.
** Development details
For working remotely, consider the ~to~ and ~from~ tasks.
Meant for usage with remote servers.
#+begin_src bash
# make changes
pixi r syncer to luthaf.cosmolab
# ssh onto remote host and build
#+end_src
*** Scratch [outdated]
Basic build:
#+begin_src bash
cd gromacs
mkdir build
cd build
cmake .. \
    -DGMX_BUILD_OWN_FFTW=ON \
    -DCMAKE_CXX_COMPILER_LAUNCHER=sccache \
    -DCMAKE_C_COMPILER_LAUNCHER=sccache \
    -DCMAKE_CXX_COMPILER=$(which clang++) \
    -DCMAKE_C_COMPILER=$(which clang) \
    -DGMX_MPI=yes \
    -DCMAKE_BUILD_TYPE=Debug
make -j$(nproc)
cmake install --install-prefix $CONDA_PREFIX
#+end_src
Easier with the ~pixi~ environment:
#+begin_src bash
pixi s -e metatomic
# Optional but recommended for logging
# cargo binstall bless
cd gromacs
mkdir build; cd $_
export TORCH_PREFIX=$(python -c "import torch; print(torch.utils.cmake_prefix_path)")
export MTS_PREFIX=$(python -c "import metatensor; print(metatensor.utils.cmake_prefix_path)")
export MTS_TORCH_PREFIX=$(python -c "import metatensor.torch; print(metatensor.torch.utils.cmake_prefix_path)")
export MTA_TORCH_PREFIX=$(python -c "import metatomic.torch; print(metatomic.torch.utils.cmake_prefix_path)")
export CMAKE_PREFIX_PATH="$TORCH_PREFIX;$MTS_PREFIX;$MTS_TORCH_PREFIX;$MTA_TORCH_PREFIX"
# drop bless -- if not installed
bless -- cmake .. \
    -DGMX_BUILD_OWN_FFTW=OFF \
    -DCMAKE_CXX_COMPILER_LAUNCHER=sccache \
    -DCMAKE_C_COMPILER_LAUNCHER=sccache \
    -DGMX_MPI=yes \
    -DGMX_METATOMIC=yes \
    -DBUILD_TESTING=OFF \
    -DCMAKE_PREFIX_PATH="$CMAKE_PREFIX_PATH" \
    -DCMAKE_BUILD_TYPE=Debug \
    -DDOWNLOAD_VESIN=yes \
    -DGPU_DEVICE=CUDA \
    -DCUDA_TOOLKIT_ROOT_DIR=$CONDA_PREFIX/targets/x86_64-linux \
    -Dnvtx3_dir=$CONDA_PREFIX/targets/x86_64-linux/include/nvtx3 \
    -DCMAKE_CXX_FLAGS="-D_LIBCPP_DISABLE_AVAILABILITY -I$CONDA_PREFIX/targets/x86_64-linux/include/ -I../src"

cmake --build . --parallel $(nproc)
cmake install . --install-prefix $CONDA_PREFIX
cmake --build . --target install
#+end_src

#+begin_src bash
# local compilers
bless -- cmake ..     -DGMX_BUILD_OWN_FFTW=OFF     -DCMAKE_CXX_COMPILER_LAUNCHER=sccache     -DCMAKE_C_COMPILER_LAUNCHER=sccache     -DGMX_MPI=OFF -DGMX_METATOMIC=yes      -DCMAKE_PREFIX_PATH="$CMAKE_PREFIX_PATH"     -DCMAKE_BUILD_TYPE=Debug     -DDOWNLOAD_VESIN=yes     -DGPU_DEVICE=CUDA  -DBUILD_TESTING=ON
make check # builds and runs tests
gmx_mpi grompp -maxwarn 10
gmx_mpi mdrun
#+end_src

mkdir -p build && \
cd build && \
export TORCH_PREFIX=$(python -c "import torch; print(torch.utils.cmake_prefix_path)") && \
export MTS_PREFIX=$(python -c "import metatensor; print(metatensor.utils.cmake_prefix_path)") && \
export MTS_TORCH_PREFIX=$(python -c "import metatensor.torch; print(metatensor.torch.utils.cmake_prefix_path)") && \
export MTA_TORCH_PREFIX=$(python -c "import metatomic.torch; print(metatomic.torch.utils.cmake_prefix_path)") && \
export CMAKE_PREFIX_PATH="$TORCH_PREFIX;$MTS_PREFIX;$MTS_TORCH_PREFIX;$MTA_TORCH_PREFIX;$CMAKE_PREFIX_PATH" && \
cmake .. \
 -DGMX_BUILD_OWN_FFTW=ON \
 -DGMX_GPU=CUDA \
 -DCMAKE_PREFIX_PATH="$CMAKE_PREFIX_PATH" \
 -DTorch_DIR=$TORCH_PREFIX/Torch \
 -DGMX_METATOMIC=ON \
 -DDOWNLOAD_VESIN=ON \
 -DCUDAToolkit_ROOT=$CONDA_PREFIX
